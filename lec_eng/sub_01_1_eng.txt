1.1.1
Econometrics tries to answer two questions: how the world works, so how one variable depends on the others, and what will happen tomorrow, so how to predict the dependent response variable y. We answer these two questions with the help of models. A model is, in simple terms, a certain formula that connects the dependent variable y and the explanatory variable x (one or several). For example, the model may look like:
yᵢ (i-th observation of the variable y) = β₁ + β₂xᵢ + εᵢ.
But before talking about evaluating models and choosing this or that model, you need to start with data, because there are no models without data. There are many different types of data in econometrics, and probably the simplest basic data types are time series, cross-sectional data and panel data. What are time series? You have some kind of time index, for example, a year, and indicators that change over time. For example, the population of Russia in 2010 was 142 thousand, almost 143, and so on, so in 2013 it was 143 thousand. And you also have the unemployment rate in percent each year respectively. That is, time series are several indicators at different points in time, ordered among themselves and usually regular – a year, a week, a month, a quarter. The second type, a very common data type, is cross-sectional data, when there are several objects, for example, several countries at the same time. For example, if we take data on the number of gold, silver and bronze medals at the 2014 Olympic Games, we will see that Russia has 13 gold, 11 silver and 9 bronze medals, and so on for other countries. That is, we have several indicators at one point in time for different objects. For example, for different individuals: we interviewed individuals, found out their salary, educational level, etc. The next type of data is more complex, it is panel data, when, for example, we would take several cross-samples related to different points in time. For example, if we took data on the results of the Olympic Games for different years, then we would get panel data. These three data types, of course, aren’t the only data types, but probably are the simplest ones, and we will start with a cross-sectional sample. We will use the following notation: by the y variable we will denote one dependent variable that is to be explained, so the variable that we want to predict or want to understand what it depends on. And explanatory variables, in other words – regressors, will be denoted by letters x, z. If there are many of them, we will number them: x₂, x₃. For each variable we will have n observations. There might be gaps in them, but in total there will be n observations for each variable. We will number them with the subscript – y₁, y₂ and so on up to yₙ. Let's consider a simple example of real historical data with a very obvious causal relationship. This is the data from the 1920s. Vehicle speeds and braking distances were measured. Here a piece of this data set is presented in the table. The braking distance in meters is 0.6 meters, 3 meters, …, and the speed, at which the car was traveling before starting to brake. Since this is the 1920s data, you won't see speeds like 60 km/h in this dataset. Here the car drove 12 km/h and slowed down within about a meter. When you're working with real data, be sure to plot the data. No amount of econometric analysis can replace simple graphical analysis. With a simple graphical analysis you can reveal some things which are very difficult to reveal econometrically, being confused blindly without graphs. If we plot our data, postponing the vehicle speed horizontally and the braking distance vertically, we will see the expected relationship: the higher the speed, the longer the braking distance is on average. So, in this case, we will assume that the model has a simple linear form. That is,
yᵢ = β₁ + β₂xᵢ + εᵢ.
We assumed that, because such a dependence is visible in our graph, – the dependence is visually similar to a linear one. What is what here? We have observable variables, y is the braking distance and x is the speed at which the car was travelling. There are unknown coefficients β₁ and β₂. That is, β₂ shows how much the braking distance increases if the car accelerates one extra kilometer per hour. And there is a certain random component ε, it can be anything, say, the driver pressed the brake in a different way or there was something else on the road, that is, this is the part which we have no way to predict, but nevertheless, here is this random error ε, it is included in y. In accordance with the available data and the questions posed, there is a certain action plan. Generally speaking, the first step is to come up with an adequate model. Well, here, judging by the picture, the linear model fits well with this dataset. Next, we need a method that, given the initial set of points, would make it possible to obtain ˆβ₁ and ˆβ₂, – the estimates of the unknown coefficients β₁ and β₂. Now, if we want to predict or interpret the coefficients, then we can predict using the estimated coefficients ˆβ₁ and ˆβ₂ instead of the unknown β₁ and β₂. There are different ways to estimate ˆβ₁ and ˆβ₂. The simplest and most popular method is the (ordinary) least squares. What is the least squares method? If we come up with some estimates for ˆβ₁ and ˆβ₂, then, naturally, we will have such a thing as a forecast error. ˆεᵢ is the difference between yᵢ, actual observation, and forecast ˆyᵢ. And, naturally, there is a total forecast error. So that the errors do not compensate each other, one plus, the other one minus, we will square them. And let's calculate the sum of the squares of the forecast errors, that is, the sum of ˆεᵢ². And the least squares method says: take as estimates such coefficients ˆβ₁ and ˆβ₂, for which the sum of the squares of the forecast errors is be minimal, that is, we minimize the sum of the squares of the forecast errors with respect to ˆβ₁ and ˆβ₂. In our example with actual data on cars, if we take actual data and substitute them in the formula for the sum of squares of forecast errors, then this sum of squares of errors will depend only on ˆβ₁ and ˆβ₂. And if we solve the minimization problem, that is, find the minimum of functions of two variables in ˆβ₁ and ˆβ₂, then we get a solution. Namely, if we do it numerically – we will do it in R – then we get ˆβ₁ = –5.3 and ˆβ₂ = 0.7. That is, we received a formula for predicting: the braking distance is –5.3 + 0.7*(by the speed of the car). We can interpret it as follows: with an increase in the speed of the car by 1 km/h, the length of the braking distance increases by 0.7 meters. And now, in order to get a better understanding of the least squares method, we will solve a couple of simple examples with numbers and specific y₁, y2, … with three observations for the simplicity of the example.

1.1.2
For two simple models on a small dataset we obtain estimates by the ordinary least squares. A simple dataset: there are 3 people: Vasya, Kolya and Petya. And there is data on their weight in kilograms and height in centimeters. And we assume that the weight will somehow depend on the height, so we will try to restore the equation of dependence. The weight is 60, 70 – for Kolya, 80 – for Petya. And the heights are 170, 170 and 181. Model 1, which we will estimate by the least squares, is
yᵢ = β + εᵢ.
And model 2, which we will estimate, is
yᵢ = β₁ + β₂xᵢ + εᵢ.
We need to obtain ˆβ using the least squares, ˆβ₁ and ˆβ₂ using the least squares. Let's talk a bit about the meaning of the models. The 1st model assumes that weight does not depend on height, that is, the weight of each person is just some kind of world constant of the average weight plus a random component that is different for each person. The second model assumes that weight depends on height linearly: β₁ + β₂x is a linear relationship, plus the random component εᵢ. And we need to evaluate these 2 models. Let’s start! So... What does the least squares method do? It minimizes the RSS value, that is, we minimize the sum of (yᵢ – ˆyᵢ)², choosing the coefficients ˆβ. Well, let's try. For model 1: ˆyᵢ… So, instead of the real coefficient we write its estimate. And a random error is unpredictable, so we write 0 instead of it. And so, if in model 1 the growth of each person is predicted by this estimated world constant, then we get that in model 1 the sum of forecast errors, – squares of forecast errors, – for i from 1 to 3 (we have 3 observations) = sum of yᵢ over i from 1 to 3. Since the forecast for each observation is ˆβ, we get such a simple formula. So that you understand that this is just an ordinary sum and there are no secrets here:
(y₁ – ˆβ)² – (y₂ – ˆβ)² + (y₃ – ˆβ)².
Let's call this function Q(ˆβ). And we take the derivative to find the maximum. No, let's first open the brackets so that it is simple and clear what this function is. This is the sum, and we open the brackets here: i from 1 to 3, (yᵢ² – 2ˆβyᵢ + ˆβ²). Or the sum over i from 1 to 3 yᵢ² minus the sum over i from 1 to 3 2ˆβyᵢ plus the sum over i from 1 to 3 ˆβ². Let's note that the last sum is the sum of the terms that do not depend on i. That is, what is the sum of ˆβ over i from 1 to 3? This is, respectively, ˆβ² + ˆβ² + ˆβ². And, accordingly, this is 3*ˆβ². In the general case, if I didn’t have 3 observations, but n observations, I would get n*ˆβ² here. The second term is the sum of 2ˆβyᵢ over i from 1 to 3. Here I see that 2ˆβ is a quantity that does not depend on the summand. With i = 1, we have 2ˆβ, the same with i = 2, and so on. And since this is a constant, it can be taken out beyond the sign of the sum as a common factor. We get 2ˆβ * the sum of yᵢ. And, accordingly, my expression for Q(ˆβ):
Q(ˆβ) = ∑yᵢ² – 2ˆβ∑yᵢ + nˆβ².
n in my example is 3, but I will additionally derive the general formula. This is a universal formula for a type 1 model for any number of observations. We take the derivative, we get that the Q prime with respect to ˆβ is equal to –2∑yᵢ + 2ˆβ. Note that our function is a parabola with upward branches with respect to ˆβ, because the coefficient at ˆβ², n, is a positive number. That is, our dependence looks like this. This is Q of ˆβ. And so the parabola has only one minimum, and that suspicious point that we find by equating the derivative to 0, it will naturally be the minimum point. That is exactly the ˆβ obtained by the least squares. Equating to 0, we get... I missed n here. Here n by ˆβ², 2ˆβ by n. Expressing ˆβ from here we get that
ˆβ = ∑yᵢ/n.
Or simply, if we sum all y and divide by n, we denote this value “y with a dash”, this is the y-mean. So, in model 1 we obtained a general formula of what ˆβ looks like. And specifically in this case we get a very natural, intuitive result: if I have 3 numbers, several numbers like 60, 70, 80, and so on, and I assume that each of these numbers is some kind of a single world constant plus random error, then the ordinary least squares says: “Well, just take the arithmetic mean as an estimate of this unknown common constant”. That is, in this model we get that ˆβ for model 1 – the least squares ˆβ – is (60 + 70 + 80) / 3 = 70.

1.1.3
Let's move on to evaluating the second, more complex model using the ordinary least squares. First we need some simple auxiliary observation to make it easier to follow the calculations. Note that if we calculate the sum of all xᵢ’s over i from 1 to n and divide it by the number of observations, we get the arithmetic mean, which we denote as x̅. So, nothing complicated. It automatically follows from this that the sum of xᵢ’s equals
∑xᵢ = nx̅,
since if we sum the same terms, there will be exactly n identical terms and it can also be said that
∑xᵢ = nx̅ = ∑x̅,
that is, the sum of xᵢ’s is equal to the sum of the same number of x-means. Since there are just n of them, this is x-mean plus x-mean plus x-mean, and so on n times. And, for example, it is possible to combine these two sums like this:
∑(xᵢ – x̅) = 0.
So, armed with such preliminary knowledge, we are ready to proceed to the evaluation of the second model. Let’s write out our residual sum of squares:
RSS = ∑(yᵢ – ŷᵢ)².
For the second model M2:
ŷᵢ = ˆβ₁ + ˆβ₂xᵢ.
εᵢ is a non-predictable part. Substituting ŷ into the residual sum of squares, we get that this is the sum over i = 1 to n,
RSS = ∑(yᵢ – ŷᵢ)² = ∑(yᵢ – ˆβ₁ – ˆβ₂xᵢ)².
This is a certain function, this function depends on ˆβ₁ and ˆβ₂,
RSS = Q(ˆβ₁,ˆβ₂).
And we will minimize this function with respect to ˆβ₁ and ˆβ₂. Let's take the derivatives straight away. If I take the derivative with respect to ˆβ₁, – ∂Q/∂ˆβ₁, – I get the sum, then 2 multiplied by the derivative of the square:
∂Q/∂ˆβ₁ = ∑2(yᵢ – ˆβ₁ – ˆβ₂xᵢ) … ,
then multiply by the derivative inside the bracket:
∂Q/∂ˆβ₁ = ∑2(yᵢ – ˆβ₁ – ˆβ₂xᵢ)(–1).
This must equal to 0: 
∂Q/∂ˆβ₁ = ∑2(yᵢ – ˆβ₁ – ˆβ₂xᵢ)(–1) = 0.
And the second equation, which I get, is ∂Q/∂ˆβ₂.
∂Q/∂ˆβ₂ = ∑2(yᵢ – ˆβ₁ – ˆβ₂xᵢ)(–xᵢ) = 0.
This is the first-order condition, and from this system of equations we already know y’s and x’s, so in these two terrible equations only ˆβ₁ and ˆβ₂ are unknown. We have two equations and two unknowns, so there is the opportunity to find a system (a set) of unknowns from such a system of equations, that is, to find ˆβ₁ and ˆβ₂. Let's reduce the equation by 2, – even by –2, – and show that these equations can be written in a very simple form, – it will be useful to us in the future. Look, this one here,
(yᵢ – ˆβ₁ – ˆβ₂xᵢ),
– this is nothing but a forecast error, that is, ˆεᵢ. So, our two equations after reduction by –2 can be rewritten in a very simple way. Sum over i from 1 to n of ˆεᵢ multiplied by 1 equals 0. Of course, you don't have to multiply by 1. And sum over i from 1 to n of ˆεᵢxᵢ equals 0. That is, our equations are written in a very simple way. Now we need to solve them. Let’s start. We are to find the coefficients ˆβ₁ and ˆβ₂. We get what remains of the first equation:
∑yᵢ – ∑ˆβ₁ – ∑ˆβ₂xᵢ = 0.
And we do the same for the second equation: opening the brackets, we obtain
∑yᵢxᵢ – ∑ˆβ₁xᵢ – ∑ˆβ₂xᵢ² = 0.
We have reduced it by –2 and expanded the parentheses. Let's divide the first equation by n, move the constants outside the sign of the sum and divide each of these equations by n, let's see what happens. Here
∑yᵢ – nˆβ₁ (because ˆβ₁ does not depend on the number of the summands) – ˆβ₂ ∑xᵢ = 0.
And here
∑yᵢxᵢ – ˆβ₁ ∑xᵢ – ˆβ₂ ∑xᵢ² = 0.
Divide the first equation by n, and it will be possible to give it one more interpretation. If the first equation is divided by n, then it turns out that
ȳ – ˆβ₁ – ˆβ₂x̅ = 0.
Or we can also say that
ȳ = ˆβ₁ + ˆβ₂x̅.
This is the first equation, from which we express ˆβ₁ and substitute it in the second equation.

1.1.4
I express ˆβ₁ and substitute it into the second equation. ˆβ₁ equals y-mean minus ˆβ₂ by x-mean, and now let’s substitute this expression for ˆβ₁ into the second equation. We get: the sum of yᵢxᵢ minus y-mean minus ˆβ₂ by x-mean multiplied by the sum of xᵢ minus ˆβ₂ by the sum of xᵢ squared equals zero. And we combine similar terms. Our task is to collect everything with ˆβ₂ and all without ˆβ₂. First, we write out everything without ˆβ₂. The sum yᵢxᵢ does not contain ˆβ₂, then minus y-mean by the sum of xᵢ. It also does not contain ˆβ₂. And now get to the terms with ˆβ₂. Plus ˆβ₂ by minus (minus, respectively, will give a plus) x-mean by the sum of xᵢ, and the same here: minus ˆβ₂ by the sum of xᵢ squared, all this equals 0. The terms without ˆβ₂ are transferred to the right side, and in the left side ˆβ₂ is taken out of the bracket: we get ˆβ₂ multiplied by x-mean minus the sum of xᵢ, oh, no – multiplied by xᵢ minus the sum of xᵢ squared. And on the right side we get y-mean by the sum of xᵢ minus the sum of yᵢxᵢ. And from this we get that ˆβ₂ equals y-mean by the sum of xᵢ minus the sum of yᵢxᵢ, divided by x-mean multiplied by the sum of xᵢ minus the sum of xᵢ squared. Let's bring this expression to a more beautiful and symmetrical form, which can be found in textbooks, that is, this is already the correct answer, but it is a little asymmetrical, it does not show the strong similarity between the numerator and the denominator. Let's first multiply the numerator and the denominator by –1 simultaneously. We will get the sum of yᵢxᵢ and then, look: y-mean is a constant, it can be put under the sum sign, and the same thing with x-mean: it is a constant and can be put under the sum sign. Minus the sum of y-mean by xᵢ, and the denominator is the sum of xᵢ squared minus the sum of x-mean by xᵢ. We put everything under one sum. The sum of yᵢ minus y-mean multiplied by xᵢ, divided by the sum of xᵢ minus x-mean by xᵢ. This is also the correct answer, but it is already a little more beautiful, it is clear that the numerator and the denominator are similar. You can also make one more transformation, which will turn the correct answer into the correct answer, but will make the numerator and the denominator of a very interesting form. Let's note that we have already said that the sum of xᵢ minus x-mean is equal to 0. Accordingly, if I multiply this sum by any indicator, absolutely by any indicator, for example, by the x-mean, then it, of course, will still equal 0. And, accordingly, we get such an interesting fact that the sum of x-mean multiplied by xᵢ minus x-mean equals 0. And to make the numerator and the denominator of the most interesting form, we will make the following transformation. In the denominator, we will subtract from the existing expression, – the sum of xᵢ minus x-mean by xᵢ, – we will subtract 0. Subtract the sum of x-mean by xᵢ minus x-mean. This is 0. Zero can be subtracted without any problems. In the denominator we'll do the same. This is the sum of yᵢ minus y-mean by xᵢ, we will also subtract zero, however, it will be a zero of a slightly different form. The sum of x-mean by yᵢ minus y-mean. This expression that we subtract is also zero. So, here is what we will get if we put everything under one sum sign: yᵢ minus y-mean will be taken out of the brackets, and the xᵢ minus x-mean will remain, and here xᵢ minus x-mean will be taken out of the brackets, and xᵢ minus x-mean will remain. And as a result, we will get another form for recording the final answer, we already have two correct ones, and we got the third correct form for writing the final answer. The sum of yᵢ minus y-mean by xᵢ minus x-mean divided by xᵢ minus x-mean squared. This form is good in the sense that the deviation of observations from the mean appears everywhere. What is xᵢ minus x-mean? This is the deviation of the height of the i-th person from the average height of the sample. What is yᵢ minus y-mean? This is the deviation of the weight of the i-th person from the mean weight of the sample, and if we were talking about deviations, then the same deviations appear everywhere in our situation. So, for our situation, if we substitute all xᵢ’s and all yᵢ’s into this formula, then we get the following estimates: ˆβ₂, according to the indicated formula, will be equal to 1.36, and knowing ˆβ₂ we can use the formula for ˆβ₁, and calculating the x-mean and y-mean we get that ˆβ₁ is –166.8. Thus, we have obtained two formulae for ˆβ₁ and ˆβ₂. Here we have a formula for ˆβ₂, this is a fraction where the deviations of the dependent and explanatory variables from their means ​are present. And also the formula for ˆβ₁, which can be calculated after the ˆβ₂ coefficient has been calculated.

1.1.5
Applying the ordinary least squares to two simple models, we obtained the following results: in a model where there are actually no explanatory variables, where
yᵢ = β + εᵢ, –
in such a model we obtained a least squares estimate: ˆβ is just y-mean, the arithmetic mean of all y’s. In the linear model, where
yᵢ = β₁ + β₂xᵢ + εᵢ,
we obtained more complex formulae. The formula for ˆβ₂ is a little cumbersome, it is difficult to directly interpret this ratio of 
∑(xᵢ – x-mean) * (yᵢ – y-mean) divided by ∑(xᵢ – x-mean)².
And in the same model
ˆβ₁ = y-mean – ˆβ₂ * x-mean.
That is, the second equation means that the regression line
ˆy = ˆβ₁ + ˆβ₂ * x
passes through the point (x-mean, y-mean). Once again, we summarize all the terminology used: yᵢ is the dependent or response variable (synonyms). xᵢ is a regressor or explanatory variable. εᵢ is simply an error, or model error, or a random component, or a random error. That is, the value that is unpredictable for us, we do not model it. ˆyᵢ (yᵢ-hat) is the forecast or the predicted value. ˆεᵢ is the residual or the forecast error. And RSS (residual sum of squares) is the sum of the squares of the residuals, the sum of the squares of the forecast errors, ∑ˆεᵢ². Let's depict our standard notations on the graph. So, we have a dataset (xᵢ, yᵢ): x₁, y₁, x₂, y₂, … and so on. A cloud of points on the plane corresponds to this set of observations. X-axis, Y-axis and some point cloud. Note that there is a certain geometric center of this cloud, this geometric center does not have to coincide with any of the points, well, it can, of course, accidentally come out like that. But in principle it doesn’t have to. That is, this certain point in the center of the cloud, from a formal mathematical point of view, is defined as x-arithmetic-mean and y-arithmetic-mean. And when we estimate the model
yᵢ = β₁ + β₂ * xᵢ + εᵢ,
we get the least squares estimate for ˆβ₁ and ˆβ₂. There is a pretty large formula for ˆβ₂, expressed in terms of the deviations
∑(xᵢ – x-mean) * (yᵢ – y-mean) divided by ∑(xᵢ – x-mean)².
And
ˆβ₁ = y-mean – ˆβ₂ * x-mean.
The second equation can also be interpreted as
ˆβ₁ + β₂ * x-mean = y-mean.
That is, we draw some straight line that replaces this point cloud. Here is some kind of a straight line. Note that this line, –
ˆy = ˆβ₁ + ˆβ₂ * x, –
must pass through the geometric center of the point cloud, because
y-mean = ˆβ₁ + ˆβ₂ * x-mean.
In addition, this line can represent the value of ˆβ₁. This value is ˆβ₁, because at x = 0, ˆy = ˆβ₁. And that is why, due to the fact that ˆβ₁ is the intersection of our regression line with the vertical axis, in English ˆβ₁ is called “intercept”. Next, let's consider some typical observation, let this be the first observation (x₁, y₁). Accordingly, for a given x, we predict such a y. This height is the real y₁, that is, if we denote it this way: it will be y₁, and this will be x₁. But this value – this will be ˆy₁ – a forecast that corresponds to the first observation. And the error is the difference between the first y and the first ˆy – this will be the forecast error ˆε₁. So, the ordinary least squares constructs the straight line so that the errors are… I will draw the errors – this is the distance from each point to the straight line, measured vertically. So, the ordinary least squares chooses a straight line so that the sum of the squares of the errors is minimal. We now turn to the case of a large number of explanatory variables. Fortunately, the case of twenty explanatory variables is conceptually no different from the case of only two explanatory variables x and z, so we will make the corresponding calculations for the model
yᵢ = β₁ + β₂xᵢ + β₃zᵢ + εᵢ.
And our task is to write out a system of equations from which ˆβ₁, ˆβ₂ and ˆβ₃ will be found. What happens if there are a lot of explanatory variables? We will consider two variables, because even with two of them the explicit formulae for ˆβ will be too cumbersome. So, we have a model
yᵢ = β₁ + β₂xᵢ + β₃zᵢ + εᵢ.
Since εᵢ is an unpredictable part, the formulae for predictions will have the form
ˆyᵢ = ˆβ₁ + ˆβ₂ * xᵢ + ˆβ₃ * zᵢ.
The forecast error ˆεᵢ equals (yᵢ – ˆyᵢ). And the ordinary least squares method minimizes the sum of ˆεᵢ squared. This quantity is a function of ˆβ₁, ˆβ₂ and ˆβ₃. We want to equate the derivative with respect to each variable to zero and see what the first-order conditions are. So, let's note that yᵢ are independent from ˆβ. Only predictions depend on ˆβ, so if I take the derivative of ˆεᵢ with respect to ˆβ₁, then yᵢ does not include a ˆβ₁, – only real β₁, which we do not know and will never know. And the derivative of ˆy with respect to ˆβ is one. So, it will be minus one. The derivative of ˆεᵢ with respect to dβ₂ for similar reasons will be equal to minus xᵢ. And the derivative of ˆεᵢ with respect to ˆβ₃ will be minus zᵢ. Well, now we can write out the first-order condition. The first-order condition will be: the derivative of Q with respect to ˆβ₁ = 0, the derivative of Q with respect to ˆβ₂ = 0, the derivative of Q with respect to ˆβ₃ = 0. Take the derivative of the sum. So, we get the first one. The sum of the derivative of the square is 2 times ˆεᵢ by the derivative of ˆε. And the derivative of ˆε is minus 1. Equals zero. We take the derivative of this condition dQ with respect to dˆβ₁ and equal it to zero. Similarly, we take the derivative of this sum with respect to ˆβ₂ and obtain the sum of 2 by ˆεᵢ by the derivative of ˆε with respect to ˆβ₂. We get minus xᵢ equal to zero. And similarly, taking the derivative of the sum of ˆεᵢ with respect to ˆβ₃, we get the sum of 2 by ˆεᵢ by minus zᵢ equal to zero. We have obtained three first-order conditions, and from this system of three equations we can find the three unknowns: ˆβ₁, ˆβ₂, and ˆβ₃. You can simplify this system a little, divide by minus 2 and you get such a nice system, from which the unknown estimates of the coefficients are found. The sum of ˆεᵢ by 1 equals zero. Again, you need not write 1, but for clarity I will. The sum of ˆεᵢ by xᵢ is zero. And the sum of ˆεᵢ by zᵢ is zero. It turns out that these conditions have a very interesting interpretation, which we will talk about right now. So, solving this system, we got the following conclusion: that the unknown estimates ˆβ₁, ˆβ₂, ˆβ₃ – the three unknown values – are found from the system of three equations. The sum of ˆεᵢ is equal to zero. The sum of the products of ˆεᵢ and xᵢ is equal to zero. And the sum of ˆεᵢ by zᵢ is also equal to zero.

1.1.6
Among other indicators that are calculated in the ordinary least squares, there are three sums of squares. These sums of squares measure the variability of several indicators. For example, the Residual Sum of Squares (RSS) is the sum of ˆεᵢ squared. And, accordingly, this indicator measures how large ˆε are, how far they lie from zero. Total Sum of Squares (TSS) is the sum of (yᵢ minus y-mean) squared. And, accordingly, this indicator measures how much each of the yᵢ’s differs from the y-mean. If yᵢ lies far from y-mean, this summand in the sum of squares will be large. And the whole sum of squares will be big. And the Explained Sum of Squares (ESS) is the sum of (ˆyᵢ minus y-mean) squared. That is, it shows how far the predicted value ˆyᵢ lies from the y-mean. The language of econometrics is in many ways a language of linear algebra, and since you have to work with data tables, with vectors, therefore the minimum amount of linear algebra is necessary for everyone, even those who have not studied linear algebra and do not know it. Moreover, it's not that difficult. Absolute basics in linear algebra. We will denote a vector with a small letter y, that is, a column of all the y’s written under each other: y₁, y₂, and so on, yₙ. Well, accordingly, small x is all x’s: x₁, x₂ and so on, xₙ, written under each other. Same with ˆε. And we will also introduce the notation: 1 with an arrow. This will be a column vector, that is, a column of 1’s, – n in total, because we will have n observations in the model. Accordingly, if our model is written without index i, so not for a separate observation, but for all observations at once, then we get that the vector ˆy is equal to ˆβ₁ multiplied by the vector column of 1’s plus ˆβ₂ multiplied by the column of x’s and plus ˆβ₃ multiplied by the column of z’s (z₁, z₂ and so on, zₙ). And we will also have another designation: a capital letter X. These will be all regressors, all explanatory variables, placed in one large table of numbers, which is called a matrix. That is, first there will be a column of 1’s, then a column of the first explanatory variable, a column of the second explanatory variable, and so on, no matter how many explanatory variables we have. Vectors have such a natural concept as length. Length of vector y. The length can be measured on a plane, it is possible in three-dimensional space, or it is possible in our n-dimensional space, since we have n observations. So, the length of the vector y is the root of the sum of the squares of the vector components. That is, it is the root of y₁ squared plus y₂ squared plus <and so on> plus yₙ squared. Well, therefore, the square of the length of the vector is the absolute value of y squared. It's just the sum of the squares of the individual y’s. The example where the length of a vector occurs is RSS. This is our sum of squares of ˆεᵢ, so it is the square of the length of vector ˆε. TSS is the sum of (yᵢ minus y-mean) squared, so it’s the square of the length of such a vector, where the first component is y₁ minus y-mean, the second component is y₂ minus y-mean, and so on. That is, TSS is the square of the length of the vector y minus y-mean times the column of 1’s. The next important simple concept is the dot product. For the dot product we know several formulae from the 9th grade. The first formula: the dot product of two vectors is the length of one multiplied by the length of the other multiplied by the cosine of the angle between them. Or the second formula for calculating the dot product: the sum of pairwise-multiplied vector coordinates. That is, x₁y₁ plus x₂y₂ plus <and so on> plus xₙyₙ. Sum of xᵢyᵢ. And the dot product, although it's hard to interpret on its own... what does the dot product equal to 42 mean? Is it big or not? We don’t know. But nevertheless, does the dot product make it easy to find out if vectors are perpendicular? Indeed: if the vectors are perpendicular, then the cosine of the 90-degrees angle between them is zero, and therefore the dot product is zero. Therefore, there is a simple criterion for the perpendicularity of two vectors: if the sum of xᵢyᵢ is zero, then the two vectors are perpendicular. And knowing these simple facts about the length of the vector and about the perpendicularity conditions allows us to illustrate the first model. Let's draw a picture in n-dimensional space for the ordinary least squares applied to the model yᵢ = β + εᵢ.

1.1.7
In models, in the initial data we usually have many observations, n can be equal to 100 or 1000. Now we will show a picture for a linear regression model in n-dimensional space, that is, in 100-dimensional or 1000-dimensional space. In order to draw in 100-dimensional space, you need a plaid scarf, like a real artist. So, we have the original data yᵢ: y₁ and so on, y₁₀₀. This is a vector, we denote it simply by letter y. Let's also consider a vector of 1’s only: 1, 1, 1... Let's draw these two vectors. The main thing here is the bold first strokes. When asked why this is how you drew a 100-dimensional vector y and a vector of 1’s, you can answer: "That’s how I see it". The fact is that we have too much freedom in 100-dimensional space to draw vectors, you can look at it from different angles, and vectors that are perpendicular at one angle can have an arbitrary angle at a different angle. Let's illustrate the model: with this simple figure illustrate a simple model
yᵢ = β + εᵢ.
Reminder: we have established that in this model ˆβ is equal to y-mean according to the ordinary least squares. So, a single forecast for the i-th observation is just the mean. And if I consider the forecast vector for all observations, then this is ˆyᵢ: ˆy₁, ˆy₂, and so on, the n-th ˆy, but since they are all the same in such a simple model, where there is actually no explanatory variable, therefore, in this model all these numbers are equal to the y-mean. The y-mean can be taken out of the vector brackets, and you get that this is the y-mean multiplied by the vector of 1’s. And this is our forecast vector. Accordingly, the forecast vector is proportional to the vector of 1’s in this model, that is, we stretch the vector of 1’s by y-mean times, so we get the y-mean vector multiplied by the vector of 1’s. Let's notice that ˆεᵢ equals (yᵢ – ˆyᵢ) – this is the forecast error. Therefore, yᵢ = ˆyᵢ + ˆεᵢ, – forecast + error. And if we write it in vector form, it means that the vector y is equal – if we add the vectors componentwise – to vector ˆy plus vector ˆε: we get the vector y. In the picture this equality means that if I connect the tip of the vector ˆy to y, then this vector will be the vector ˆε. Indeed, you can just go through the vector y, or you can go through the vector ˆy and then add ˆε. But that is not all. Let's note that we have the first-order conditions: we have ˆyᵢ = y-mean, that is, to put it another way, we know that the sum of ˆyᵢ equals the sum of y-means, equals n times y-mean, equals the sum of yᵢ. That is, it turns out that the sum of ˆyᵢ is equal to the sum of yᵢ-mean, and subtracting one from the other, we get that the sum of (yᵢ – ˆyᵢ) is equal to 0, that is, the sum of ˆεᵢ by 1 equals 0, and this condition can be interpreted geometrically. This condition means that the vector ˆε is perpendicular to the vector of 1’s. We got a wonderful result. The result as follows, – the interpretation of this simple model: if I need to estimate the model yᵢ = β + εᵢ using the least squares, then in order to find ˆβ, it turns out that this can be done geometrically in n-dimensional space, where n is the number of observations. Namely, I take a vector y, find its projection onto a straight line, which is given by a vector of 1’s, and the result of the projection will be the vector ˆy. Again. I have a vector y, I have a vector of 1’s, I continue this vector to a straight line and it turns out that in a simple model without regressors, projecting the vector y onto this straight line results in a vector ˆy. Along the way we got another fact: if any vector is projected onto a vector of 1’s, then we get a vector of mean values. y-mean and so on, y-mean. Well, for example, if I have three vectors in 100-dimensional space – a vector y, a vector of 1’s and a vector x, if I project a vector y onto a vector of 1’s, I get vector y-mean, y-mean, y-mean, and if I project the vector x onto the same vector of 1’s, then I will get, – considering that the vector x is no different from the vector y, – I will get a vector of x-mean, x-mean, x-mean, and so on, x-mean.

1.1.8
Now let's look at the same situation where we have a lot of regressors. Let me remind you that we have derived first-order conditions which are:
∑ˆεᵢ * 1 = 0,
∑ˆεᵢ * xᵢ = 0
and 
∑ˆεᵢ * zᵢ = 0.
Geometrically this means that vector ˆε is perpendicular to the vector of 1’s, ˆε is perpendicular to vector x, and ˆε is perpendicular to vector z. So, we can illustrate the least squares method for the case of many explanatory variables. At the same time, in this figure, we will see where the TSS, RSS and ESS are. Now we will illustrate the case of multiple regression in a multidimensional space, that is, we will illustrate the estimate of the model
yᵢ = β₁ + β₂ * xᵢ + β₃ * zᵢ + εᵢ
using the ordinary least squares. Let me remind you that we found out that the first-order condition for estimating the unknown coefficients β₁, β₂ and β₃ is as follows: ˆε should be perpendicular to the vector of 1’s, ˆε should be perpendicular to vector x and ˆε should be perpendicular to vector z. This is the first fact that can be easily illustrated geometrically. The second fact... Yes, from this system of equations we find ˆβ₁, ˆβ₂ and ˆβ₃ – the estimates of the unknowns β₁, β₂ and β₃. In addition, we know that ˆεᵢ is the forecast error, (yᵢ – ŷ). Hence, it turns out that
yᵢ = ŷᵢ + ˆεᵢ,
or in vector form: vector y = vector ŷ + vector ˆε, and this fact, of course, is also perfectly illustrated geometrically. This means that you can just go through vector y, or you can go through vector ŷ and then through vector ˆε and you will finish at the same point. Now let’s illustrate it without hurry. In addition, perhaps it is worth noting that ŷ = ˆβ₁ * vector of 1’s + ˆβ₂ * vector x + ˆβ₃ * vector z. That is, vector ŷ can be expressed by adding (with some weights) the vector of 1’s, vector x and vector z. Now let’s draw. Here we have a vector of 1’s. This is its continuation, it sets some straight line. Here we will have the origin. Let me remind you that our space is n-dimensional, n is the number of observations. Therefore... And n is usually large. For example, 100 observations. Therefore, for example, in this space, we can draw 20 vectors perpendicular to each other, and all of them will be pairwise perpendicular to each other. This is vector x. This is vector z. This will be vector y. Firstly, I will designate with such a cloud the entire set of vectors that can be obtained by adding (with some weight) vector x, vector z and the vector of 1’s. That is, the cloud is all those vectors that can be obtained by adding vector x, vector z and the vector of 1’s with some weights. So, how do I geometrically interpret the first-order conditions and the fact that ˆε is the difference between y and ŷ? First, ŷ can be expressed in terms of the vector of 1’s, in terms of vector x and in terms of vector z. Here it is, the formula expressing it. Hence, it lies in this cloud. In this – in clever words – hyperplane. So, vector ŷ. Let it be of a different color. This is the vector ŷ. Note that vector ˆε, together with vector ŷ, results in vector y. In addition, since this plane, – this hyperplane, this cloud – is formed by the vector of 1’s, vector x and vector z, and since vector ˆε is perpendicular to all of them, so ˆε is perpendicular to the entire hyperplane, to the entire cloud, and, accordingly, here we get the right angle. Thus, we got the following interpretation of the least squares: ŷ – the predictions that we get by the least squares – are the projection of the original vector of dependent variables y onto the set of vectors obtained by adding a vector of 1’s, vector x, and vector z with different weights. This is the first geometric factor. Let’s note some other geometric factors. First, note that from the first-order condition that ˆε is perpendicular to the vector of 1’s, we get that
∑ˆεᵢ = 0,
or the fact that
∑yᵢ – ŷᵢ = 0,
or that 
∑yᵢ = ∑ŷᵢ,
or ȳ (y-mean) = ŷ-mean. That is, the mean of actual dependent variables is equal to the mean of the predictions. But we know that the mean value can be obtained by projecting any vector onto a vector... onto a straight line generated by the vector of 1’s. Accordingly, this fact geometrically means the following: if I project the vector y onto the line generated by the vector of 1’s, and project ŷ onto the same line, then these projections will fall into the same point. Well, back in school this fact in the 11th grade was called the “theorem of three perpendiculars”. This is where it miraculously appears. Here I got a vector ŷ multiplied by the vector of 1’s, or rather not a vector... a number ŷ multiplied by the vector of 1’s, and here there will be the same projection. And let's look at the resulting triangle with vertices at the tip of vector ŷ, the vertex at the tip... the vertex at the tip of vector y, the vertex at the tip of vector ŷ, and the vertex at the tip of the stretched vector of 1’s. So we got a triangle. This triangle is rectangular. Let me name the points for convenience. A, B, C. This triangle is right-angled because ˆε is perpendicular to everything in the cloud, and BC is in the cloud, so this triangle is right-angled. And, accordingly, the Pythagorean theorem works here, namely that the hypotenuse – this will be our third geometric fact – namely that AB² = AC² + BC². Let’s look at the meaning of each indicator, how we interpret it econometrically. AC is... AC² ​​is the length of the vector ˆε², this is ∑ˆεᵢ², this is RSS. AB²… Well, that's the length of the vector AB squared. Vector AB is the difference between y and ȳ multiplied by the vector of 1’s. Accordingly, it is
| y – ȳ * the vector of 1’s |².
So, it is
∑(yᵢ – ȳ)²,
and so it is TSS. Finally, BC² is the length of the vector BC. BC is the difference between the two vectors – the vector ŷ and the stretched vector of 1’s. Accordingly, it is
| ŷ – ȳ * vector of 1’s |².
So, it is
∑(ŷ – ȳ)²,
and so it is ESS. Thus, we have proved that the following relationship holds between RSS, TSS and ESS (namely, the Pythagorean theorem): TSS = ESS + RSS. But the wonders of geometric interpretation do not end there. Let's look at the fourth fact, namely that the ratio of ESS divided by TSS is the ratio of the square of this leg to the length of the hypotenuse. So the ratio of ESS to TSS, which we will interpret further, is the cosine squared, this is the ratio... ESS (this is BC²) divided by TSS (this is AB²) is (BC / AB) ², and this is the ratio of the leg to the hypotenuse, this is the cosine of the angle, so, this is the squared cosine of the angle φ, (cosφ)². We need it and I will immediately show its geometric meaning. Hence, the angle between BC and AB is the angle φ. And the cosine of this angle is the ratio of ESS to TSS. And these four geometric facts are very convenient to operate in order to understand the meaning of the least squares. Namely, ŷ is the projection of y onto the set of vectors generated by the regressors. The second fact is that ȳ = ŷ-mean. This is due to the first condition, namely the fact that we include the intercept in the model, from here, when we take the derivative with respect to ˆβ₁, we get this condition. And thanks to this fact, we get that TSS = ESS + RSS. The Pythagorean theorem is valid. And the ratio of ESS to TSS, which does have its meaning, turns out to be the squared cosine of some angle, (cosφ)².

1.1.9
We have illustrated the ordinary least squares method for multiple regression when we have many regressors. And along the way we discovered the following facts: if an intercept is included in the model, that is, yᵢ = β₁ + ..., (plus explanatory regressors), that is, if there is β₁, then among the columns of the matrix X there will be a column of 1’s, and in this case our estimates of the least squares will have the following properties: the sum of the forecast errors, ∑ˆεᵢ = 0; the sum of y’s will equal the sum of the predicted y’s: ∑yᵢ = ∑ŷ, or, in other words, the mean of the forecast will be equal to the mean of the observed variable, ȳ = ŷ-mean. And the decomposition TSS = RSS + ESS will also hold, in the figure we saw this as the Pythagorean theorem. The presence of the last decomposition – the decomposition of the total sum of squares into two components, RSS + ESS – allows us to come up with a simple indicator of the quality of the model, namely the R-squared (R²), which is called the coefficient of determination in clever words. We know that the more closely the forecasts resemble the real y, the smaller the errors of the forecasts ˆε, and the smaller the sum of the squares of the forecast errors, namely RSS. Accordingly, ESS / TSS, or R², will be approximately equal to 1 if RSS (sum of squared errors) is around zero. So, we got the coefficient R², the coefficient of determination, which always lies from 0 to 1 (R² ∈ [0; 1]), and its values ​​close to 0 mean that the sum of squares of errors, – the residual sum of squares is very large. If R² is close to 1, this indicates that the residual sum of squares, – the sum of the squares of the forecast errors is small. In other words, R² is the proportion of the explained variation of y to the total variation, the total sum of squares. The R² coefficient can also be interpreted in a different way. Not only as the explained fraction of the spread of y, but also as the square of the sample correlation, namely: R² is the square of the sample correlation coefficient, which is equal to the following fraction: in the numerator there is
∑(yᵢ – ȳ) * (ŷᵢ – ȳ)
and in the denominator there is the root of TSS multiplied by the root of ESS. It turns out that the proof of this fact is very simple. The fact is that the sample correlation coefficient is nothing else but the cosine of the angle between some two vectors, and we will now see it. This is where we need our fourth fact that
ESS / TSS = (cosφ)².
As we found out, the coefficient of determination is the ratio of ESS to TSS:
R² = ESS / TSS.
This means that it is the proportion of the explained variation of the y variable to the total variation of the y variable. Accordingly, from the previous formulae it follows that this indicator lies between 0 and 1 (R² ∈ [0; 1]) and R² = (cosφ)². But it turns out that we have another formula for the cosine. Due to the fact that the scalar product of any two vectors a and b is the length of the vector a multiplied by the length of the vector b and by the cosine of the angle between them –
(a, b) = |a| * |b| * cos(a, b)
– this fact implies the expression for the cosine. Namely: the cosine between any two vectors is the scalar product of these two vectors divided by the product of the lengths of the two vectors:
cos(a, b) = (a, b) / |a| * |b|.
Accordingly, what in our case will be R², or the squared cosine between which two vectors will it be? What angle is it cosine of? Angle φ is the angle between AB and BC. Our vector AB, within the accuracy of direction, can be specified as y – ȳ by the vector of 1’s. So… It's actually BA. Vector. Vector BC can be specified as a vector ŷ – ȳ by the vector of 1’s. This is the BC vector. And we need the cosine of the angle between these vectors. What do we get when we calculate the length of the first vector and the length of the second vector? We get that the scalar product of BC and BA is the scalar product of the vector (y – ȳ * the vector of 1’s) and (ŷ – ȳ * the vector of 1’s). Well, this is the sum, – since the dot product can be defined as... the dot product (a, b) can be defined as ∑aᵢ*bᵢ, – so we just need to multiply the components of these vectors pairwise. So we get: the first component of this vector is (y₁ – ȳ), the first component of the second vector is (ŷ₁ – ȳ). I take (yᵢ – ȳ) and multiply by (ŷᵢ – ȳ). I found the numerator. And what does the denominator look like? In the denominator we have the length of the vector BC – it is the square root of the sum of the squares of its components, that is, it is the square root of ∑(ŷᵢ – ȳ)². The length of the vector BA is the root of the sum of the squares of its components. That is the square root of ∑(yᵢ – ȳ)². So, we got another formula for the coefficient R², namely:
R² = ∑ (yᵢ – ȳ) * (ŷᵢ – ȳ) divided by the square root of ∑ (yᵢ – ȳ) ² * ∑ (ŷᵢ – ȳ) ².
What is the meaning of it? Here you can recognize the formula of the sample correlation between two vectors, namely: this is the sample correlation – I'll denote it as sCorr – sample correlation between vectors y and ŷ – sCorr(y, ŷ). And, of course, everything needs to be squared, since I counted the cosine of the angle, and R² is the squared cosine of the angle. Thus, we got two interpretations for the indicator of the quality of a regression. On the one hand, R² is the fraction of the explained variance of y, the fraction of the explained variation of y. This is the explained spread of y divided by the total spread of y. On the other hand, R² is the sample correlation between the forecasts and the actual y squared. The higher the R², the more ŷ looks like y. The higher the R², the higher the percentage of the explained variance.

1.1.10
We have found out that the (ordinary) least squares method allows us to obtain estimates of the coefficients in a linear model. In this case, the least squares estimates are obtained as a solution to a certain system of linear equations. There is a whole science that partly deals with the solution of systems of linear equations – it is linear algebra. And using the methods of linear algebra, one can show that the estimates ˆβ, using the terminology of linear algebra, have the following form: it is (X-transposed X) to the power of minus 1 multiplied by X-transposed y. So, having a ready-made model, we can estimate it using the ordinary least squares. We have not yet covered many issues. Firstly, where do we get the model from? Secondly, even if we have a model, will there be a solution to the minimization problem, will there be unique estimates, maybe there will be several variants of different ˆβ’s? Will there be a solution to the problem at all? Why did we take the sum of the squares of residuals, and not the sum of, say, absolute values? And how close are the obtained estimates ˆβ to the real unknown β’s? In the following lessons we will try to answer all these questions.